{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af87e88f",
   "metadata": {},
   "source": [
    "# Setup Guide\n",
    "\n",
    "Complete guide to set up the Steam Analytics Platform.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Microsoft Fabric workspace (F64 or higher recommended)\n",
    "- Steam API Key\n",
    "- Azure Service Principal (for local development)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Get Steam API Key\n",
    "\n",
    "1. Go to [Steam API Key Registration](https://steamcommunity.com/dev/apikey)\n",
    "2. Log in with your Steam account\n",
    "3. Register a new API key (domain name can be \"localhost\" for testing)\n",
    "4. Save the key securely\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Create Fabric Resources\n",
    "\n",
    "### 2.1 Create Lakehouses\n",
    "\n",
    "Create 3 lakehouses in your Fabric workspace:\n",
    "\n",
    "| Lakehouse | Purpose |\n",
    "|-----------|---------|\n",
    "| `lh_bronze` | Raw data storage |\n",
    "| `lh_silver` | Cleaned, transformed data |\n",
    "| `lh_gold` | Aggregated business data |\n",
    "\n",
    "**Steps:**\n",
    "1. Go to your Fabric workspace\n",
    "2. Click **New** → **Lakehouse**\n",
    "3. Name it `lh_bronze`\n",
    "4. Repeat for `lh_silver` and `lh_gold`\n",
    "\n",
    "### 2.2 Create Fabric Environment\n",
    "\n",
    "Create a custom environment with required Python libraries:\n",
    "\n",
    "1. In workspace, click **New** → **Environment**\n",
    "2. Name: `env_steam_analytics`\n",
    "3. Go to **Public libraries** tab\n",
    "4. Add the following packages:\n",
    "```\n",
    "azure-identity==1.25.1\n",
    "azure-storage-file-datalake==12.23.0\n",
    "httpx==0.28.1\n",
    "pydantic==2.12.5\n",
    "pydantic-settings==2.12.0\n",
    "structlog==25.5.0\n",
    "tenacity==9.1.2\n",
    "```\n",
    "\n",
    "5. Click **Publish** and wait for environment to build (~5 minutes)\n",
    "\n",
    "### 2.3 Upload Python Package\n",
    "\n",
    "1. Open `lh_bronze` lakehouse\n",
    "2. Navigate to **Files** section\n",
    "3. Create folder structure: `Files/src/steam_analytics/`\n",
    "4. Upload the entire `src/steam_analytics/` folder\n",
    "```\n",
    "lh_bronze/\n",
    "└── Files/\n",
    "    └── src/\n",
    "        └── steam_analytics/\n",
    "            ├── __init__.py\n",
    "            ├── config.py\n",
    "            ├── catalog/\n",
    "            ├── ingestion/\n",
    "            └── transformation/\n",
    "```\n",
    "\n",
    "### 2.4 Import Notebooks\n",
    "\n",
    "1. In workspace, click **New** → **Import notebook**\n",
    "2. Import all notebooks from the `notebooks/` folder:\n",
    "   - `00_catalog_discovery.ipynb`\n",
    "   - `00_catalog_refresh.ipynb`\n",
    "   - `00_monitor.ipynb`\n",
    "   - `01_bronze_ingestion.ipynb`\n",
    "   - `02_silver_transform.ipynb`\n",
    "   - `03_gold_transform.ipynb`\n",
    "\n",
    "3. Attach environment and lakehouse to each notebook:\n",
    "   - Environment: `env_steam_analytics`\n",
    "   - Default lakehouse: `lh_bronze`\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Configure Notebooks\n",
    "\n",
    "Update environment variables in each notebook's first cell:\n",
    "```python\n",
    "import os\n",
    "\n",
    "os.environ[\"STEAM__API_KEY\"] = \"your_steam_api_key\"\n",
    "os.environ[\"FABRIC__WORKSPACE_ID\"] = \"your_workspace_id\"\n",
    "os.environ[\"FABRIC__BRONZE_LAKEHOUSE_ID\"] = \"your_bronze_lakehouse_id\"\n",
    "os.environ[\"FABRIC__SILVER_LAKEHOUSE_ID\"] = \"your_silver_lakehouse_id\"\n",
    "os.environ[\"FABRIC__GOLD_LAKEHOUSE_ID\"] = \"your_gold_lakehouse_id\"\n",
    "\n",
    "# Only for Bronze ingestion (OneLake writes)\n",
    "os.environ[\"AZURE_TENANT_ID\"] = \"your_tenant_id\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = \"your_client_id\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = \"your_client_secret\"\n",
    "```\n",
    "\n",
    "### Finding Lakehouse IDs\n",
    "\n",
    "1. Open any lakehouse\n",
    "2. Look at the URL: `https://app.fabric.microsoft.com/groups/{workspace_id}/lakehouses/{lakehouse_id}`\n",
    "3. Copy the `workspace_id` and `lakehouse_id` from the URL\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Initial Data Load\n",
    "\n",
    "### 4.1 Run Catalog Discovery (One-time)\n",
    "\n",
    "This populates the game catalog with ~150,000 games. Takes 1-2 hours.\n",
    "\n",
    "1. Open `00_catalog_discovery.ipynb`\n",
    "2. Run all cells\n",
    "3. Verify catalog created: `lh_bronze/Tables/game_catalog`\n",
    "\n",
    "### 4.2 Run Full Pipeline\n",
    "\n",
    "Execute notebooks in order:\n",
    "```\n",
    "1. 00_catalog_refresh.ipynb    → Detect new games\n",
    "2. 01_bronze_ingestion.ipynb   → Extract from Steam APIs\n",
    "3. 02_silver_transform.ipynb   → Transform to Silver\n",
    "4. 03_gold_transform.ipynb     → Aggregate to Gold\n",
    "```\n",
    "\n",
    "### 4.3 Verify Data\n",
    "\n",
    "Run `00_monitor.ipynb` to check:\n",
    "```\n",
    "PIPELINE HEALTH CHECK\n",
    "============================================================\n",
    "\n",
    "BRONZE LAYER\n",
    "  ✅ game_catalog: 150,000+ records\n",
    "  ✅ raw_steam_store: X records\n",
    "  ✅ raw_steam_reviews: X records\n",
    "  ✅ raw_steam_player_stats: X records\n",
    "\n",
    "SILVER LAYER\n",
    "  ✅ dim_games: X records\n",
    "  ✅ dim_game_reviews: X records\n",
    "  ✅ fact_player_counts: X records\n",
    "\n",
    "GOLD LAYER\n",
    "  ✅ agg_game_metrics: X records\n",
    "  ✅ agg_price_history: X records\n",
    "  ✅ agg_genre_summary: X records\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Create Data Pipeline\n",
    "\n",
    "### 5.1 Create Pipeline\n",
    "\n",
    "1. In workspace, click **New** → **Data Pipeline**\n",
    "2. Name: `pl_steam_analytics_daily`\n",
    "\n",
    "### 5.2 Add Activities\n",
    "\n",
    "Add 4 Notebook activities in sequence:\n",
    "```\n",
    "┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐\n",
    "│ Catalog_Refresh │───►│ Bronze_Ingestion│───►│ Silver_Transform│───►│ Gold_Transform  │\n",
    "└─────────────────┘    └─────────────────┘    └─────────────────┘    └─────────────────┘\n",
    "```\n",
    "\n",
    "| Activity | Notebook | Timeout |\n",
    "|----------|----------|---------|\n",
    "| Catalog_Refresh | 00_catalog_refresh | 30 min |\n",
    "| Bronze_Ingestion | 01_bronze_ingestion | 120 min |\n",
    "| Silver_Transform | 02_silver_transform | 30 min |\n",
    "| Gold_Transform | 03_gold_transform | 30 min |\n",
    "\n",
    "### 5.3 Configure Dependencies\n",
    "\n",
    "For each activity (except first):\n",
    "- Add dependency: **On Success** from previous activity\n",
    "\n",
    "### 5.4 Schedule Pipeline\n",
    "\n",
    "1. Click **Schedule** in pipeline toolbar\n",
    "2. Configure:\n",
    "   - Frequency: Daily\n",
    "   - Time: 06:00 UTC\n",
    "   - Start date: Today\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Create Power BI Dashboard\n",
    "\n",
    "### 6.1 Create Semantic Model\n",
    "\n",
    "1. Open `lh_gold` lakehouse\n",
    "2. Click **New semantic model**\n",
    "3. Name: `sm_steam_analytics`\n",
    "4. Select tables:\n",
    "   - ☑️ agg_game_metrics\n",
    "   - ☑️ agg_price_history\n",
    "   - ☑️ agg_genre_summary\n",
    "\n",
    "### 6.2 Create Relationships\n",
    "\n",
    "In semantic model view:\n",
    "- Drag `agg_game_metrics[app_id]` to `agg_price_history[app_id]`\n",
    "- Cardinality: One to Many (1:*)\n",
    "\n",
    "### 6.3 Create Report\n",
    "\n",
    "1. From semantic model, click **New report**\n",
    "2. Build 3 pages:\n",
    "   - Overview\n",
    "   - Price History\n",
    "   - Genre Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**\"Module not found\" error in notebook:**\n",
    "- Verify `src/` folder is uploaded to `lh_bronze/Files/`\n",
    "- Check `sys.path.insert(0, \"/lakehouse/default/Files\")` is in first cell\n",
    "\n",
    "**API rate limit errors:**\n",
    "- Reduce concurrency in extractors\n",
    "- Check Steam API key is valid\n",
    "\n",
    "**OneLake write failures:**\n",
    "- Verify Azure credentials are correct\n",
    "- Check Service Principal has Storage Blob Data Contributor role\n",
    "\n",
    "**Environment not loading:**\n",
    "- Wait for environment to finish publishing\n",
    "- Detach and re-attach environment to notebook\n",
    "\n",
    "---\n",
    "\n",
    "## Local Development (Optional)\n",
    "\n",
    "For testing locally without Fabric:\n",
    "```bash\n",
    "# Clone repository\n",
    "git clone https://github.com/yourusername/steam-analytics.git\n",
    "cd steam-analytics\n",
    "\n",
    "# Create virtual environment\n",
    "python -m venv .venv\n",
    "source .venv/bin/activate  # Linux/Mac\n",
    ".venv\\Scripts\\activate     # Windows\n",
    "\n",
    "# Install package\n",
    "pip install -e \".[dev]\"\n",
    "\n",
    "# Set environment variables\n",
    "export STEAM__API_KEY=\"your_key\"\n",
    "# ... other variables\n",
    "\n",
    "# Run tests\n",
    "pytest\n",
    "\n",
    "# Run extraction locally (outputs to local files)\n",
    "python -m steam_analytics.cli extract --app-ids 730 570 --output ./data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556dcde",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
