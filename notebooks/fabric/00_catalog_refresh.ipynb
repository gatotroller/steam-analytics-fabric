{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420651c-f4ce-4607-acf6-d414c4813dab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"STEAM__API_KEY\"] = \"id\"\n",
    "os.environ[\"FABRIC__WORKSPACE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__BRONZE_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__SILVER_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__GOLD_LAKEHOUSE_ID\"] = \"id\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/lakehouse/default/Files\")\n",
    "\n",
    "from src.steam_analytics.config import Settings\n",
    "settings = Settings()\n",
    "\n",
    "BRONZE_PATH = settings.fabric.bronze_abfss_path\n",
    "CATALOG_PATH = f\"{BRONZE_PATH}/Tables/game_catalog\"\n",
    "\n",
    "print(f\"Catalog Path: {CATALOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d3aa5-7a55-44c2-93c6-ee8fc764061c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Loading existing catalog...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "existing_catalog = spark.read.format(\"delta\").load(CATALOG_PATH)\n",
    "existing_count = existing_catalog.count()\n",
    "\n",
    "print(f\"Existing games in catalog: {existing_count:,}\")\n",
    "\n",
    "# Get set of existing app_ids for fast lookup\n",
    "existing_ids = set(\n",
    "    row.app_id for row in \n",
    "    existing_catalog.select(\"app_id\").collect()\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(existing_ids):,} app IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cbf1a2-cc6a-4fde-928b-9dc51607e478",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from src.steam_analytics.ingestion.extractors.app_list import AppListExtractor\n",
    "\n",
    "extractor = AppListExtractor()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: Fetching current app list from Steam...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_apps = await extractor.get_all_apps()\n",
    "filtered_apps = extractor.filter_likely_games(all_apps)\n",
    "\n",
    "print(f\"Total games from Steam: {len(filtered_apps):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cf65a-dcc0-4481-84c4-0ae01a278a07",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: Detecting new games...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "new_apps = [app for app in filtered_apps if app.app_id not in existing_ids]\n",
    "\n",
    "print(f\"New games found: {len(new_apps):,}\")\n",
    "\n",
    "if new_apps:\n",
    "    print(\"\\nSample of new games:\")\n",
    "    for app in new_apps[:10]:\n",
    "        print(f\"  {app.app_id}: {app.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f8d8e-2217-40e2-96bd-41134a134b9f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "if new_apps:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 4: Fetching player counts for new games...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    new_app_ids = [app.app_id for app in new_apps]\n",
    "    \n",
    "    player_counts = await extractor.get_player_counts_batch(\n",
    "        new_app_ids,\n",
    "        concurrency=5\n",
    "    )\n",
    "    \n",
    "    player_lookup = {\n",
    "        r.app_id: r.player_count \n",
    "        for r in player_counts \n",
    "        if r.success\n",
    "    }\n",
    "    \n",
    "    print(f\"Got player counts for {len(player_lookup):,} new games\")\n",
    "else:\n",
    "    print(\"No new games to process!\")\n",
    "    player_lookup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b953d-04e8-4305-91fe-91aef8eb2cda",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from src.steam_analytics.catalog import GameCatalogManager\n",
    "from datetime import datetime\n",
    "\n",
    "if new_apps:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 5: Creating catalog entries for new games...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    manager = GameCatalogManager()\n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    new_entries = []\n",
    "    for app in new_apps:\n",
    "        player_count = player_lookup.get(app.app_id)\n",
    "        entry = manager.create_catalog_entry(\n",
    "            app_id=app.app_id,\n",
    "            name=app.name,\n",
    "            player_count=player_count,\n",
    "            discovered_at=now,\n",
    "        )\n",
    "        new_entries.append(entry.to_dict())\n",
    "    \n",
    "    # Stats\n",
    "    from collections import Counter\n",
    "    priority_counts = Counter(e[\"priority\"] for e in new_entries)\n",
    "    \n",
    "    print(f\"New entries created: {len(new_entries):,}\")\n",
    "    print(\"\\nBy priority:\")\n",
    "    for priority, count in sorted(priority_counts.items()):\n",
    "        print(f\"  {priority}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71797c-fa66-4b95-a872-793de9ef2cc8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "if new_apps and new_entries:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 6: Appending new games to catalog (Explicit Schema Fixed)...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. EL CAMBIO CLAVE:\n",
    "    # Definimos las fechas como StringType() inicialmente porque tus datos vienen como texto \"2026-...\"\n",
    "    catalog_schema = StructType([\n",
    "        StructField(\"app_id\", IntegerType(), False),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"player_count\", IntegerType(), True),\n",
    "        StructField(\"priority\", StringType(), True),\n",
    "        StructField(\"discovered_at\", StringType(), True),   # <--- AHORA ES STRING\n",
    "        StructField(\"last_synced_at\", StringType(), True)   # <--- AHORA ES STRING\n",
    "    ])\n",
    "    \n",
    "    # 2. Creamos el DataFrame (Ahora Spark aceptará el texto sin quejarse)\n",
    "    new_df = spark.createDataFrame(new_entries, schema=catalog_schema)\n",
    "    \n",
    "    # 3. Convertimos a Timestamp ANTES de guardar\n",
    "    # Esto transforma el texto \"2026-02-05...\" a un objeto de fecha real\n",
    "    new_df_final = (new_df\n",
    "        .withColumn(\"discovered_at\", F.col(\"discovered_at\").cast(TimestampType()))\n",
    "        .withColumn(\"last_synced_at\", F.col(\"last_synced_at\").cast(TimestampType()))\n",
    "    )\n",
    "    \n",
    "    # 4. Guardamos\n",
    "    new_df_final.write.format(\"delta\").mode(\"append\").save(CATALOG_PATH)\n",
    "    \n",
    "    print(f\"✅ Appended {len(new_entries):,} new games to catalog successfully!\")\n",
    "\n",
    "else:\n",
    "    print(\"Nothing to append - catalog is up to date!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136955ee-6efa-4144-b274-464f74be8541",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_catalog = spark.read.format(\"delta\").load(CATALOG_PATH)\n",
    "final_count = final_catalog.count()\n",
    "\n",
    "print(f\"Previous catalog size: {existing_count:,}\")\n",
    "print(f\"New games added:       {final_count - existing_count:,}\")\n",
    "print(f\"Final catalog size:    {final_count:,}\")\n",
    "\n",
    "print(\"\\nFinal Priority Distribution:\")\n",
    "final_catalog.groupBy(\"priority\").agg(\n",
    "    F.count(\"*\").alias(\"games\"),\n",
    "    F.sum(\"player_count\").alias(\"total_players\")\n",
    ").orderBy(\"priority\").show()\n",
    "\n",
    "print(\"\\nCatalog refresh complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9eb52-f582-4fcc-a4e0-f05803266c76",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "import json\n",
    "\n",
    "output = {\n",
    "    \"status\": \"success\",\n",
    "    \"previous_count\": existing_count,\n",
    "    \"new_games_added\": final_count - existing_count,\n",
    "    \"final_count\": final_count,\n",
    "}\n",
    "\n",
    "print(json.dumps(output, indent=2))\n",
    "mssparkutils.notebook.exit(json.dumps(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de985cc8-dd2a-499b-8155-ce8bfc5c51c5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "ce58e1bf-2df1-46fd-8bf9-c58aa87264ab",
    "workspaceId": "25bfefc0-562b-4cfc-9853-63d527854451"
   },
   "lakehouse": {
    "default_lakehouse": "313303be-1bc9-416a-aab3-a7d44ed60e29",
    "default_lakehouse_name": "lh_bronze",
    "default_lakehouse_workspace_id": "25bfefc0-562b-4cfc-9853-63d527854451",
    "known_lakehouses": [
     {
      "id": "313303be-1bc9-416a-aab3-a7d44ed60e29"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
