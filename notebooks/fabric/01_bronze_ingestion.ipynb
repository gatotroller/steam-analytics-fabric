{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2db5168-0b2d-4275-93cc-4fd34ee89028",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 01 - Bronze Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4e09f-276d-43bf-9e88-ed7dc3409af0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Config\n",
    "os.environ[\"AZURE_TENANT_ID\"] = \"id\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = \"id\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = \"id\"\n",
    "\n",
    "os.environ[\"STEAM__API_KEY\"] = \"id\"\n",
    "os.environ[\"FABRIC__WORKSPACE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__BRONZE_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__SILVER_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__GOLD_LAKEHOUSE_ID\"] = \"id\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/lakehouse/default/Files\")\n",
    "\n",
    "from src.steam_analytics.config import Settings\n",
    "settings = Settings()\n",
    "\n",
    "BRONZE_PATH = settings.fabric.bronze_abfss_path\n",
    "CATALOG_PATH = f\"{BRONZE_PATH}/Tables/game_catalog\"\n",
    "\n",
    "print(f\"Bronze Path: {BRONZE_PATH}\")\n",
    "print(f\"Catalog Path: {CATALOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a12050-a167-4204-8dba-653bbce271e3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "# Si ya tienes las variables cargadas, ignora estas lÃ­neas de config\n",
    "# from src.steam_analytics.config import Settings\n",
    "# settings = Settings()\n",
    "# BRONZE_PATH = settings.fabric.bronze_abfss_path\n",
    "# ... etc\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ§¹ INICIANDO VACIADO DE TABLAS (MANTENIENDO ESTRUCTURA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lista de tablas a vaciar\n",
    "tables_to_truncate = [\n",
    "    f\"{BRONZE_PATH}/Tables/game_catalog\",\n",
    "]\n",
    "\n",
    "for table_path in tables_to_truncate:\n",
    "    try:\n",
    "        # 1. Conectamos con la tabla Delta en esa ruta\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)\n",
    "        \n",
    "        # 2. Ejecutamos DELETE sin condiciones (Borra todo)\n",
    "        delta_table.delete()\n",
    "        \n",
    "        # 3. Verificamos que quedÃ³ vacÃ­a\n",
    "        count = delta_table.toDF().count()\n",
    "        print(f\"âœ… {table_path.split('/')[-1]}: Vaciada correctamente. Filas restantes: {count}\")\n",
    "        \n",
    "    except AnalysisException:\n",
    "        # Esto pasa si la tabla no existe todavÃ­a\n",
    "        print(f\"âš ï¸ {table_path.split('/')[-1]}: No existe o no es una tabla Delta vÃ¡lida.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error en {table_path.split('/')[-1]}: {str(e)}\")\n",
    "\n",
    "print(\"\\nâœ¨ Proceso terminado. Las tablas estÃ¡n vacÃ­as y listas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94931c65-54e8-40bf-98ab-d2d1f252dc9f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from src.steam_analytics.catalog import GameCatalogManager, SyncPriority\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CATALOG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load catalog\n",
    "catalog_df = spark.read.format(\"delta\").load(CATALOG_PATH)\n",
    "total_in_catalog = catalog_df.count()\n",
    "print(f\"Total games in catalog: {total_in_catalog:,}\")\n",
    "\n",
    "# Determine priorities based on day of week\n",
    "manager = GameCatalogManager()\n",
    "day_of_week = datetime.utcnow().weekday()\n",
    "day_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "\n",
    "priorities = manager.get_sync_schedule(day_of_week)\n",
    "priority_values = [p.value for p in priorities]\n",
    "\n",
    "print(f\"\\nToday: {day_names[day_of_week]}\")\n",
    "print(f\"Priorities to sync: {priority_values}\")\n",
    "\n",
    "# Filter catalog by priority\n",
    "apps_to_sync_df = catalog_df.filter(\n",
    "    F.col(\"priority\").isin(priority_values)\n",
    ")\n",
    "\n",
    "# Get app IDs\n",
    "APP_IDS = [row.app_id for row in apps_to_sync_df.select(\"app_id\").collect()]\n",
    "\n",
    "print(f\"\\nGames to sync today: {len(APP_IDS):,}\")\n",
    "\n",
    "# Show breakdown\n",
    "print(\"\\nBreakdown by priority:\")\n",
    "apps_to_sync_df.groupBy(\"priority\").count().orderBy(\"priority\").show()\n",
    "\n",
    "# Estimate time\n",
    "estimate = manager.estimate_sync_time(len(APP_IDS))\n",
    "print(f\"Estimated time: ~{estimate['estimated_minutes']:.0f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafdec5-b2c8-4204-b9fd-17078187e68d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from src.steam_analytics.ingestion.orchestrator import IngestionOrchestrator, OutputTarget\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING INGESTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Games to process: {len(APP_IDS):,}\")\n",
    "\n",
    "orchestrator = IngestionOrchestrator(target=OutputTarget.ONELAKE)\n",
    "result = await orchestrator.run(APP_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1dd93-8fda-4804-be99-4d04facfae11",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPDATING CATALOG (DELTA MERGE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get successfully synced app IDs\n",
    "synced_ids = APP_IDS  # O usa result.processed_ids si tu orquestador lo devuelve\n",
    "\n",
    "if synced_ids:\n",
    "    now = datetime.utcnow()\n",
    "    \n",
    "    # 1. Crear un DataFrame pequeÃ±o SOLO con las actualizaciones (Updates)\n",
    "    # Esto es mucho mÃ¡s rÃ¡pido que leer todo el catÃ¡logo de 150k filas\n",
    "    updates_data = [{\"app_id\": app_id, \"last_synced_at\": now} for app_id in synced_ids]\n",
    "    \n",
    "    updates_schema = StructType([\n",
    "        StructField(\"app_id\", IntegerType(), False),\n",
    "        StructField(\"last_synced_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    updates_df = spark.createDataFrame(updates_data, schema=updates_schema)\n",
    "    \n",
    "    # 2. Instanciar la tabla Delta destino\n",
    "    target_table = DeltaTable.forPath(spark, CATALOG_PATH)\n",
    "    \n",
    "    # 3. Ejecutar el MERGE\n",
    "    (target_table.alias(\"target\")\n",
    "        .merge(\n",
    "            updates_df.alias(\"source\"),\n",
    "            \"target.app_id = source.app_id\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"last_synced_at\": \"source.last_synced_at\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Efficiently merged updates for {len(synced_ids):,} games\")\n",
    "\n",
    "else:\n",
    "    print(\"No updates to merge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb53db5-3439-4849-9c81-e597f56458a1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57e839-149f-4599-b96e-82c7d9c0afd0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BRONZE INGESTION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Run ID:         {result.run_id}\")\n",
    "print(f\"Duration:       {result.duration_seconds:.2f}s\")\n",
    "print(f\"Batches:        {len(result.batches_written)}\")\n",
    "print(f\"Success Rate:   {result.success_rate}%\")\n",
    "print(f\"Games Synced:   {len(APP_IDS):,}\")\n",
    "\n",
    "if result.errors:\n",
    "    print(f\"Errors:         {len(result.errors)}\")\n",
    "    for err in result.errors[:5]:\n",
    "        print(f\"  - {err}\")\n",
    "\n",
    "# Exit with status\n",
    "mssparkutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"success\" if result.success_rate >= 80.0 else \"partial\",\n",
    "    \"run_id\": str(result.run_id),\n",
    "    \"batches_written\": len(result.batches_written),\n",
    "    \"success_rate\": result.success_rate,\n",
    "    \"games_synced\": len(APP_IDS),\n",
    "}, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cd072-4225-4730-a2a7-1de41bcd756d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "ce58e1bf-2df1-46fd-8bf9-c58aa87264ab",
    "workspaceId": "25bfefc0-562b-4cfc-9853-63d527854451"
   },
   "lakehouse": {
    "default_lakehouse": "313303be-1bc9-416a-aab3-a7d44ed60e29",
    "default_lakehouse_name": "lh_bronze",
    "default_lakehouse_workspace_id": "25bfefc0-562b-4cfc-9853-63d527854451",
    "known_lakehouses": [
     {
      "id": "313303be-1bc9-416a-aab3-a7d44ed60e29"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
