{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2db5168-0b2d-4275-93cc-4fd34ee89028",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 01 - Bronze Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4e09f-276d-43bf-9e88-ed7dc3409af0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Config\n",
    "os.environ[\"AZURE_TENANT_ID\"] = \"id\"\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = \"id\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = \"id\"\n",
    "\n",
    "os.environ[\"STEAM__API_KEY\"] = \"id\"\n",
    "os.environ[\"FABRIC__WORKSPACE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__BRONZE_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__SILVER_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__GOLD_LAKEHOUSE_ID\"] = \"id\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/lakehouse/default/Files\")\n",
    "\n",
    "from src.steam_analytics.config import Settings\n",
    "settings = Settings()\n",
    "\n",
    "BRONZE_PATH = settings.fabric.bronze_abfss_path\n",
    "CATALOG_PATH = f\"{BRONZE_PATH}/Tables/game_catalog\"\n",
    "\n",
    "print(f\"Bronze Path: {BRONZE_PATH}\")\n",
    "print(f\"Catalog Path: {CATALOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94931c65-54e8-40bf-98ab-d2d1f252dc9f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Load Catalog with Staleness Check\n",
    "from pyspark.sql import functions as F\n",
    "from src.steam_analytics.catalog import GameCatalogManager, SyncPriority\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING CATALOG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "catalog_df = spark.read.format(\"delta\").load(CATALOG_PATH)\n",
    "total_in_catalog = catalog_df.count()\n",
    "print(f\"Total games in catalog: {total_in_catalog:,}\")\n",
    "\n",
    "# Determine priorities based on day of week\n",
    "manager = GameCatalogManager()\n",
    "now = datetime.utcnow()\n",
    "day_of_week = now.weekday()\n",
    "day_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "\n",
    "priorities = manager.get_sync_schedule(day_of_week)\n",
    "priority_values = [p.value for p in priorities]\n",
    "\n",
    "print(f\"\\nToday: {day_names[day_of_week]}\")\n",
    "print(f\"Priorities to sync: {priority_values}\")\n",
    "\n",
    "# Filter by priority\n",
    "apps_df = catalog_df.filter(F.col(\"priority\").isin(priority_values))\n",
    "\n",
    "stale_conditions = (\n",
    "    # Never synced\n",
    "    (F.col(\"last_synced_at\").isNull()) |\n",
    "    # HIGH: stale after 1 day\n",
    "    ((F.col(\"priority\") == \"high\") & \n",
    "     (F.col(\"last_synced_at\") < F.lit(now - timedelta(days=1)))) |\n",
    "    # MEDIUM: stale after 7 days\n",
    "    ((F.col(\"priority\") == \"medium\") & \n",
    "     (F.col(\"last_synced_at\") < F.lit(now - timedelta(days=7)))) |\n",
    "    # LOW: stale after 30 days\n",
    "    ((F.col(\"priority\") == \"low\") & \n",
    "     (F.col(\"last_synced_at\") < F.lit(now - timedelta(days=30))))\n",
    ")\n",
    "\n",
    "apps_to_sync_df = apps_df.filter(stale_conditions)\n",
    "\n",
    "# Get app IDs\n",
    "APP_IDS = [row.app_id for row in apps_to_sync_df.select(\"app_id\").collect()]\n",
    "\n",
    "print(f\"\\nGames to sync (stale): {len(APP_IDS):,}\")\n",
    "\n",
    "# Breakdown\n",
    "print(\"\\nBreakdown:\")\n",
    "apps_to_sync_df.groupBy(\"priority\").count().orderBy(\"priority\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafdec5-b2c8-4204-b9fd-17078187e68d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from src.steam_analytics.ingestion.orchestrator import IngestionOrchestrator, OutputTarget\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RUNNING INGESTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Games to process: {len(APP_IDS):,}\")\n",
    "\n",
    "orchestrator = IngestionOrchestrator(target=OutputTarget.ONELAKE)\n",
    "result = await orchestrator.run(APP_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1dd93-8fda-4804-be99-4d04facfae11",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UPDATING CATALOG (DELTA MERGE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get successfully synced app IDs\n",
    "synced_ids = APP_IDS  # O usa result.processed_ids si tu orquestador lo devuelve\n",
    "\n",
    "if synced_ids:\n",
    "    now = datetime.now(timezone.utc)\n",
    "    \n",
    "    # 1. Crear un DataFrame pequeño SOLO con las actualizaciones (Updates)\n",
    "    # Esto es mucho más rápido que leer todo el catálogo de 150k filas\n",
    "    updates_data = [{\"app_id\": app_id, \"last_synced_at\": now} for app_id in synced_ids]\n",
    "    \n",
    "    updates_schema = StructType([\n",
    "        StructField(\"app_id\", IntegerType(), False),\n",
    "        StructField(\"last_synced_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    updates_df = spark.createDataFrame(updates_data, schema=updates_schema)\n",
    "    \n",
    "    # 2. Instanciar la tabla Delta destino\n",
    "    target_table = DeltaTable.forPath(spark, CATALOG_PATH)\n",
    "    \n",
    "    # 3. Ejecutar el MERGE\n",
    "    (target_table.alias(\"target\")\n",
    "        .merge(\n",
    "            updates_df.alias(\"source\"),\n",
    "            \"target.app_id = source.app_id\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"last_synced_at\": \"source.last_synced_at\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"Efficiently merged updates for {len(synced_ids):,} games\")\n",
    "\n",
    "else:\n",
    "    print(\"No updates to merge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b52b0e-f3ae-4fac-80f0-29c98152c8c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"UPDATING PRIORITIES FROM FRESH DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read latest player counts from Bronze\n",
    "player_stats_path = f\"{BRONZE_PATH}/Tables/raw_steam_player_stats\"\n",
    "\n",
    "try:\n",
    "    # Get today's player counts\n",
    "    today_str = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    latest_players = spark.read.format(\"delta\").load(player_stats_path) \\\n",
    "        .filter(F.col(\"ingestion_date\") == today_str) \\\n",
    "        .select(\"app_id\", \"player_count\") \\\n",
    "        .dropDuplicates([\"app_id\"])\n",
    "    \n",
    "    fresh_count = latest_players.count()\n",
    "    print(f\"Fresh player counts available: {fresh_count:,}\")\n",
    "    \n",
    "    if fresh_count > 0:\n",
    "        # Read current catalog\n",
    "        catalog_df = spark.read.format(\"delta\").load(CATALOG_PATH)\n",
    "        \n",
    "        # Join with fresh player counts\n",
    "        updated_catalog = catalog_df.alias(\"c\").join(\n",
    "            latest_players.alias(\"p\"),\n",
    "            F.col(\"c.app_id\") == F.col(\"p.app_id\"),\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            F.col(\"c.app_id\"),\n",
    "            F.col(\"c.name\"),\n",
    "            # Use fresh player_count if available, else keep old\n",
    "            F.coalesce(F.col(\"p.player_count\"), F.col(\"c.player_count\")).alias(\"player_count\"),\n",
    "            F.col(\"c.discovered_at\"),\n",
    "            F.col(\"c.last_synced_at\"),\n",
    "            # Recalculate priority based on new player count\n",
    "            F.when(F.coalesce(F.col(\"p.player_count\"), F.col(\"c.player_count\")) >= 1000, \"high\")\n",
    "             .when(F.coalesce(F.col(\"p.player_count\"), F.col(\"c.player_count\")) >= 100, \"medium\")\n",
    "             .when(F.coalesce(F.col(\"p.player_count\"), F.col(\"c.player_count\")) >= 1, \"low\")\n",
    "             .otherwise(\"skip\").alias(\"priority\")\n",
    "        )\n",
    "        \n",
    "        # Check for priority changes\n",
    "        priority_changes = updated_catalog.alias(\"new\").join(\n",
    "            catalog_df.alias(\"old\"),\n",
    "            F.col(\"new.app_id\") == F.col(\"old.app_id\")\n",
    "        ).filter(\n",
    "            F.col(\"new.priority\") != F.col(\"old.priority\")\n",
    "        ).count()\n",
    "        \n",
    "        print(f\"Priority changes detected: {priority_changes:,}\")\n",
    "        \n",
    "        # Save updated catalog\n",
    "        updated_catalog.write.format(\"delta\").mode(\"overwrite\").save(CATALOG_PATH)\n",
    "        print(\"✅ Catalog priorities updated!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not update priorities: {e}\")\n",
    "    print(\"Continuing without priority update...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb53db5-3439-4849-9c81-e597f56458a1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57e839-149f-4599-b96e-82c7d9c0afd0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BRONZE INGESTION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Run ID:         {result.run_id}\")\n",
    "print(f\"Duration:       {result.duration_seconds:.2f}s\")\n",
    "print(f\"Batches:        {len(result.batches_written)}\")\n",
    "print(f\"Success Rate:   {result.success_rate}%\")\n",
    "print(f\"Games Synced:   {len(APP_IDS):,}\")\n",
    "\n",
    "if result.errors:\n",
    "    print(f\"Errors:         {len(result.errors)}\")\n",
    "    for err in result.errors[:5]:\n",
    "        print(f\"  - {err}\")\n",
    "\n",
    "# Exit with status\n",
    "mssparkutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"success\" if result.success_rate >= 80.0 else \"partial\",\n",
    "    \"run_id\": str(result.run_id),\n",
    "    \"batches_written\": len(result.batches_written),\n",
    "    \"success_rate\": result.success_rate,\n",
    "    \"games_synced\": len(APP_IDS),\n",
    "}, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4b89a-1564-4ee1-a89d-a6173d15fc45",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "ce58e1bf-2df1-46fd-8bf9-c58aa87264ab",
    "workspaceId": "25bfefc0-562b-4cfc-9853-63d527854451"
   },
   "lakehouse": {
    "default_lakehouse": "313303be-1bc9-416a-aab3-a7d44ed60e29",
    "default_lakehouse_name": "lh_bronze",
    "default_lakehouse_workspace_id": "25bfefc0-562b-4cfc-9853-63d527854451",
    "known_lakehouses": [
     {
      "id": "313303be-1bc9-416a-aab3-a7d44ed60e29"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
