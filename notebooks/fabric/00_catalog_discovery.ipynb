{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95caf581-de87-49b3-a2d9-3282d926ac12",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"STEAM__API_KEY\"] = \"id\"\n",
    "os.environ[\"FABRIC__WORKSPACE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__BRONZE_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__SILVER_LAKEHOUSE_ID\"] = \"id\"\n",
    "os.environ[\"FABRIC__GOLD_LAKEHOUSE_ID\"] = \"id\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/lakehouse/default/Files\")\n",
    "\n",
    "from src.steam_analytics.config import Settings\n",
    "settings = Settings()\n",
    "\n",
    "BRONZE_PATH = settings.fabric.bronze_abfss_path\n",
    "print(f\"Bronze Path: {BRONZE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa78648-4c1d-4a71-829e-197d2576c613",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Fetch All Games from Steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f1171-6096-4f4c-914b-7b8108a55b7b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from src.steam_analytics.ingestion.extractors.app_list import AppListExtractor\n",
    "\n",
    "extractor = AppListExtractor()\n",
    "\n",
    "# Get all games (uses IStoreService with include_games=true)\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: Fetching game list from Steam API...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_apps = await extractor.get_all_apps()\n",
    "print(f\"\\nTotal apps from API: {len(all_apps):,}\")\n",
    "\n",
    "# Additional filtering (in case API lets some junk through)\n",
    "filtered_apps = extractor.filter_likely_games(all_apps)\n",
    "print(f\"After keyword filter: {len(filtered_apps):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44bc9f-27f8-49f2-861b-7f17d19589e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Get Player Counts (This takes ~1-2 hours)\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: Fetching player counts...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Games to check: {len(filtered_apps):,}\")\n",
    "print(\"Estimated time: ~1-2 hours\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get all app IDs\n",
    "app_ids = [app.app_id for app in filtered_apps]\n",
    "\n",
    "# Progress callback for logging\n",
    "def progress(completed, total):\n",
    "    if completed % 5000 == 0:\n",
    "        pct = completed / total * 100\n",
    "        print(f\"Progress: {completed:,}/{total:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Fetch player counts (rate limited)\n",
    "player_counts = await extractor.get_player_counts_batch(\n",
    "    app_ids, \n",
    "    concurrency=5,\n",
    "    progress_callback=progress\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted! Got {len(player_counts):,} results\")\n",
    "\n",
    "# Create lookup\n",
    "player_lookup = {\n",
    "    r.app_id: r.player_count \n",
    "    for r in player_counts \n",
    "    if r.success\n",
    "}\n",
    "print(f\"Successful lookups: {len(player_lookup):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7912ed-cfb0-474c-87af-f00c7735a3f6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Build Catalog with Priorities\n",
    "from src.steam_analytics.catalog import GameCatalogManager, SyncPriority\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: Building catalog with priorities...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "manager = GameCatalogManager()\n",
    "now = datetime.utcnow()\n",
    "\n",
    "catalog_entries = []\n",
    "for app in filtered_apps:\n",
    "    player_count = player_lookup.get(app.app_id)\n",
    "    \n",
    "    entry = manager.create_catalog_entry(\n",
    "        app_id=app.app_id,\n",
    "        name=app.name,\n",
    "        player_count=player_count,\n",
    "        discovered_at=now,\n",
    "    )\n",
    "    catalog_entries.append(entry.to_dict())\n",
    "\n",
    "print(f\"Catalog entries created: {len(catalog_entries):,}\")\n",
    "\n",
    "# Convert to DataFrame for stats\n",
    "import pandas as pd\n",
    "catalog_pdf = pd.DataFrame(catalog_entries)\n",
    "\n",
    "# Show priority distribution\n",
    "print(\"\\nPriority Distribution:\")\n",
    "print(catalog_pdf[\"priority\"].value_counts().sort_index())\n",
    "\n",
    "# Show top games\n",
    "print(\"\\nTop 10 Games by Player Count:\")\n",
    "top_10 = catalog_pdf.nlargest(10, \"player_count\")[[\"app_id\", \"name\", \"player_count\", \"priority\"]]\n",
    "print(top_10.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c9a43-5b8b-4339-9715-95ba5c8649aa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, TimestampType, LongType\n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: Saving catalog to Delta table...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "catalog_schema = StructType([\n",
    "    StructField(\"app_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"player_count\", IntegerType(), True),\n",
    "    StructField(\"priority\", StringType(), True),\n",
    "    StructField(\"discovered_at\", StringType(), False),  # Viene como string ISO\n",
    "    StructField(\"last_synced_at\", StringType(), True)   # Viene como None, lo tratamos como String temporalmente\n",
    "])\n",
    "\n",
    "# Convert to Spark DataFrame usando el esquema\n",
    "catalog_df = spark.createDataFrame(catalog_entries, schema=catalog_schema)\n",
    "\n",
    "# Add metadata (El resto de tu código sigue igual...)\n",
    "catalog_df = catalog_df.withColumn(\n",
    "    \"discovered_at\", \n",
    "    F.to_timestamp(F.col(\"discovered_at\"))\n",
    ").withColumn(\n",
    "    \"last_synced_at\",\n",
    "    F.lit(None).cast(TimestampType()) # Aquí ya lo convertimos a Timestamp oficial\n",
    ")\n",
    "\n",
    "# Show schema\n",
    "catalog_df.printSchema()\n",
    "\n",
    "# Show stats\n",
    "print(\"\\nCatalog Stats:\")\n",
    "catalog_df.groupBy(\"priority\").count().orderBy(\"priority\").show()\n",
    "\n",
    "# Total players\n",
    "total_players = catalog_df.agg(F.sum(\"player_count\")).collect()[0][0]\n",
    "print(f\"Total players across all games: {total_players:,}\")\n",
    "\n",
    "# Save to Delta\n",
    "catalog_path = f\"{BRONZE_PATH}/Tables/game_catalog\"\n",
    "print(f\"\\nSaving to: {catalog_path}\")\n",
    "\n",
    "catalog_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(catalog_path)\n",
    "\n",
    "print(\"✅ Catalog saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9ac39-300d-4436-8339-3ff68e53c666",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Verify\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read back\n",
    "verify_df = spark.read.format(\"delta\").load(f\"{BRONZE_PATH}/Tables/game_catalog\")\n",
    "\n",
    "print(f\"Total games in catalog: {verify_df.count():,}\")\n",
    "\n",
    "print(\"\\nBy Priority:\")\n",
    "verify_df.groupBy(\"priority\").agg(\n",
    "    F.count(\"*\").alias(\"games\"),\n",
    "    F.sum(\"player_count\").alias(\"total_players\"),\n",
    "    F.avg(\"player_count\").alias(\"avg_players\")\n",
    ").orderBy(\"priority\").show()\n",
    "\n",
    "print(\"\\nTop 20 Games:\")\n",
    "verify_df.orderBy(F.col(\"player_count\").desc()).select(\n",
    "    \"app_id\", \"name\", \"player_count\", \"priority\"\n",
    ").show(20, truncate=False)\n",
    "\n",
    "print(\"\\n✅ Discovery complete!\")\n",
    "print(f\"Catalog ready at: {BRONZE_PATH}/Tables/game_catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff740a8-be8d-4055-8add-0816e875a93d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Estimate Sync Times\n",
    "print(\"=\" * 60)\n",
    "print(\"SYNC TIME ESTIMATES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get counts per priority\n",
    "high_count = verify_df.filter(\"priority = 'high'\").count()\n",
    "medium_count = verify_df.filter(\"priority = 'medium'\").count()\n",
    "low_count = verify_df.filter(\"priority = 'low'\").count()\n",
    "\n",
    "print(f\"\\nHIGH priority:   {high_count:,} games\")\n",
    "print(f\"MEDIUM priority: {medium_count:,} games\")\n",
    "print(f\"LOW priority:    {low_count:,} games\")\n",
    "\n",
    "print(\"\\nDaily Sync Estimates (3 API sources per game):\")\n",
    "for name, count in [(\"HIGH only\", high_count), \n",
    "                     (\"HIGH+MEDIUM\", high_count + medium_count),\n",
    "                     (\"ALL\", high_count + medium_count + low_count)]:\n",
    "    est = manager.estimate_sync_time(count)\n",
    "    print(f\"  {name}: ~{est['estimated_minutes']:.0f} min ({est['estimated_hours']:.1f} hrs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f14724-e68a-42a8-8d7f-07a4fc6e7551",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "ce58e1bf-2df1-46fd-8bf9-c58aa87264ab",
    "workspaceId": "25bfefc0-562b-4cfc-9853-63d527854451"
   },
   "lakehouse": {
    "default_lakehouse": "313303be-1bc9-416a-aab3-a7d44ed60e29",
    "default_lakehouse_name": "lh_bronze",
    "default_lakehouse_workspace_id": "25bfefc0-562b-4cfc-9853-63d527854451",
    "known_lakehouses": [
     {
      "id": "313303be-1bc9-416a-aab3-a7d44ed60e29"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
